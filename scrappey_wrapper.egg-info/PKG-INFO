Metadata-Version: 2.4
Name: scrappey-wrapper
Version: 1.0.0
Summary: A drop-in replacement wrapper for ScrapFly SDK using Scrappey API
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: httpx>=0.25.0
Requires-Dist: parsel>=1.8.0
Provides-Extra: dev
Requires-Dist: loguru; extra == "dev"
Requires-Dist: jmespath; extra == "dev"
Requires-Dist: nested-lookup; extra == "dev"
Dynamic: license-file

# ScrapFly Scrapers - Scrappey Wrapper üï∑Ô∏è

> **Fork of [scrapfly/scrapfly-scrapers](https://github.com/scrapfly/scrapfly-scrapers)** modified to use [Scrappey](https://scrappey.com) API instead of ScrapFly.

This repository contains 46+ educational example scrapers for popular web scraping targets, originally built for ScrapFly, now adapted to work with the Scrappey web scraping API.

## Why Scrappey Over ScrapFly?

| Feature | Scrappey | ScrapFly |
|---------|----------|----------|
| **Anti-Bot Bypass** | ‚úÖ Built-in by default, no extra config needed | ‚öôÔ∏è Requires `asp=True` flag |
| **Browser Rendering** | ‚úÖ Real browser by default (Chromium-based) | ‚öôÔ∏è Requires `render_js=True` |
| **Cloudflare Bypass** | ‚úÖ Automatic Cloudflare, Akamai, PerimeterX bypass | ‚úÖ Available with ASP |
| **CAPTCHA Solving** | ‚úÖ Automatic CAPTCHA solving included | üí∞ Additional cost |
| **Session Management** | ‚úÖ Persistent sessions with cookies/fingerprints | ‚úÖ Available |
| **Pricing** | üí∞ More affordable for high-volume scraping | üí∞ Premium pricing |
| **API Simplicity** | ‚úÖ Simple JSON-based REST API | ‚öôÔ∏è SDK required |
| **Residential Proxies** | ‚úÖ Premium residential with `premiumProxy: true` | ‚úÖ Available |

### Key Advantages of Scrappey

1. **Zero Configuration Anti-Bot** - Scrappey handles anti-bot protection automatically. No need to set flags or worry about detection - it just works out of the box.

2. **Real Browser by Default** - Every request runs through a real Chromium browser instance, ensuring JavaScript-heavy sites render correctly without extra configuration.

3. **Cost Effective** - Better pricing structure for production workloads, especially when you need CAPTCHA solving and premium proxies.

4. **Simpler Integration** - Pure REST API with JSON payloads means you can use any HTTP client. No SDK lock-in required.

5. **Automatic Fingerprint Rotation** - Browser fingerprints are automatically rotated to avoid detection patterns.

## What's Changed

This fork replaces the ScrapFly SDK with a custom `scrappey_wrapper` module that provides a drop-in replacement for ScrapFly's API. The wrapper translates ScrapFly's interface to Scrappey's API format.

### Changes Made

1. **Created `scrappey_wrapper/` module** - A Python package that mimics ScrapFly's SDK interface:
   - `ScrappeyClient` (aliased as `ScrapflyClient`) - Main client for making API requests
   - `ScrapeConfig` - Configuration class for scrape requests
   - `ScrapeApiResponse` - Response wrapper with `.selector`, `.content`, `.context` properties
   - Custom exception classes for error handling

2. **Updated all 46+ scrapers** to use the new wrapper:
   - Changed imports from `from scrapfly import ...` to `from scrappey_wrapper import ...`
   - Updated environment variable from `SCRAPFLY_KEY` to `SCRAPPEY_KEY`

3. **API Parameter Mapping**:
   | ScrapFly | Scrappey |
   |----------|----------|
   | `asp=True` | Handled by default (anti-bot bypass) |
   | `country="US"` | `proxyCountry: "UnitedStates"` |
   | `proxy_pool="public_residential_pool"` | `premiumProxy: true` |
   | `render_js=True` | Browser rendering by default |
   | `wait_for_selector` | `browserActions` with `waitForSelector` |
   | `headers={}` | `customHeaders` |
   | `method="POST"` | `cmd: "request.post"` |
   | `body=data` | `postData` |

## Quick Start

### 1. Install the Package

```bash
# Clone the repository
git clone https://github.com/pim97/scrapfly-scrapers-scrappey-wrapper.git
cd scrapfly-scrapers-scrappey-wrapper

# Install the scrappey_wrapper package (required)
pip install -e .

# Install additional dependencies for scrapers
pip install loguru jmespath nested-lookup
```

### 2. Set Your Scrappey API Key

```bash
# Linux/macOS
export SCRAPPEY_KEY="your-scrappey-api-key"

# Windows (Command Prompt)
set SCRAPPEY_KEY=your-scrappey-api-key

# Windows (PowerShell)
$env:SCRAPPEY_KEY="your-scrappey-api-key"
```

### 3. Run Any Scraper

```bash
# Example: Run Amazon scraper
python amazon-scraper/run.py

# Example: Run Google scraper
python google-scraper/run.py
```

## Scrappey Wrapper Architecture

```
scrappey_wrapper/
‚îú‚îÄ‚îÄ __init__.py        # Module exports
‚îú‚îÄ‚îÄ scrappey.py        # ScrappeyClient with async_scrape(), concurrent_scrape()
‚îú‚îÄ‚îÄ config.py          # ScrapeConfig dataclass
‚îú‚îÄ‚îÄ response.py        # ScrapeApiResponse wrapper
‚îî‚îÄ‚îÄ exceptions.py      # Custom exception classes
```

## High Concurrency Support

Scrappey supports up to **100 concurrent requests** by default (vs ScrapFly's lower limits). This makes large-scale scraping significantly faster.

```python
from scrappey_wrapper import ScrapflyClient

# Default: 100 concurrent requests
client = ScrapflyClient(key="your-api-key")

# Or customize concurrency (1-100)
client = ScrapflyClient(
    key="your-api-key",
    max_concurrency=50,      # Limit to 50 concurrent requests
    timeout=120,             # Request timeout in seconds
    max_retries=3,           # Retry transient errors
)
```

### Concurrent Scraping Example

```python
import asyncio
from scrappey_wrapper import ScrapflyClient, ScrapeConfig

client = ScrapflyClient(max_concurrency=100)  # Full speed!

urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    # ... hundreds of URLs
]

async def scrape_all():
    configs = [ScrapeConfig(url=url) for url in urls]
    
    async for response in client.concurrent_scrape(configs):
        if hasattr(response, 'selector'):
            # Success - process the response
            print(f"Scraped: {response.url}")
        else:
            # Error - handle it
            print(f"Error: {response}")

asyncio.run(scrape_all())
```

### Usage Example

```python
import os
from scrappey_wrapper import ScrapeConfig, ScrapflyClient, ScrapeApiResponse

# Initialize client
client = ScrapflyClient(key=os.environ["SCRAPPEY_KEY"])

# Create scrape config
config = ScrapeConfig(
    url="https://example.com",
    asp=True,           # Anti-bot (handled by Scrappey automatically)
    country="US",       # Proxy country
    render_js=True,     # Browser rendering
)

# Async scrape
async def scrape():
    response = await client.async_scrape(config)
    html = response.content
    selector = response.selector
    data = selector.css("h1::text").get()
    return data
```

## Tech Stack

- Python 3.10+
- [httpx](https://www.python-httpx.org/) for async HTTP requests
- [parsel](https://pypi.org/project/parsel/) for HTML parsing (XPath/CSS selectors)
- [asyncio](https://docs.python.org/3/library/asyncio.html) for concurrent scraping
- [loguru](https://pypi.org/project/loguru/) for logging

## List of Scrapers

Below is the list of available web scrapers (46+ domains):

| Domain | Guide |
|--------|-------|
| [Aliexpress.com](./aliexpress-scraper/) | [How to Scrape Aliexpress.com](https://scrapfly.io/blog/how-to-scrape-aliexpress/) |
| [Amazon.com](./amazon-scraper/) | [How to Scrape Amazon.com](https://scrapfly.io/blog/how-to-scrape-amazon/) |
| [BestBuy.com](./bestbuy-scraper/) | [How to Scrape BestBuy](https://scrapfly.io/blog/how-to-scrape-bestbuy-product-offer-and-review-data/) |
| [Bing.com](./bing-scraper/) | [How to Scrape Bing Search](https://scrapfly.io/blog/how-to-scrape-bing-search-using-python/) |
| [Booking.com](./bookingcom-scraper/) | [How to Scrape Booking.com](https://scrapfly.io/blog/how-to-scrape-bookingcom/) |
| [Crunchbase.com](./crunchbase-scraper/) | [How to Scrape Crunchbase](https://scrapfly.io/blog/how-to-scrape-crunchbase/) |
| [Domain.com.au](./domaincom-scraper/) | [How to Scrape Domain.com.au](https://scrapfly.io/blog/how-to-scrape-domain-com-au-real-estate-property-data/) |
| [Ebay.com](./ebay-scraper/) | [How to Scrape Ebay](https://scrapfly.io/blog/how-to-scrape-ebay/) |
| [Etsy.com](./etsy-scraper/) | [How to Scrape Etsy.com](https://scrapfly.io/blog/how-to-scrape-etsy-com-product-review-data/) |
| [Fashionphile.com](./fashionphile-scraper/) | [How to Scrape Fashionphile](https://scrapfly.io/blog/how-to-scrape-fashionphile/) |
| [G2.com](./g2-scraper/) | [How to Scrape G2](https://scrapfly.io/blog/how-to-scrape-g2-company-data-and-reviews/) |
| [Glassdoor.com](./glassdoor-scraper/) | [How to Scrape Glassdoor](https://scrapfly.io/blog/how-to-scrape-glassdoor/) |
| [Goat.com](./goat-scraper/) | [How to Scrape Goat.com](https://scrapfly.io/blog/how-to-scrape-goat-com-fashion-apparel/) |
| [Google.com](./google-scraper/) | [How to Scrape Google](https://scrapfly.io/blog/how-to-scrape-google/) |
| [Homegate.ch](./homegate-scraper/) | [How to Scrape Homegate.ch](https://scrapfly.io/blog/how-to-scrape-homegate-ch-real-estate-property-data/) |
| [Idealista.com](./idealista-scraper/) | [How to Scrape Idealista](https://scrapfly.io/blog/how-to-scrape-idealista/) |
| [Immobilienscout24.de](./immobilienscout24-scraper/) | [How to Scrape Immobilienscout24.de](https://scrapfly.io/blog/how-to-scrape-immobillienscout24-real-estate-property-data/) |
| [Immoscout24.ch](./immoscout24-scraper/) | [How to Scrape Immoscout24.ch](https://scrapfly.io/blog/how-to-scrape-immoscout24-ch-real-estate-property-data/) |
| [Immowelt.de](./immowelt-scraper/) | [How to Scrape Immowelt.de](https://scrapfly.io/blog/how-to-scrape-immowelt-de-real-estate-properties/) |
| [Indeed.com](./indeed-scraper/) | [How to Scrape Indeed.com](https://scrapfly.io/blog/how-to-scrape-indeedcom/) |
| [Instagram.com](./instagram-scraper/) | [How to Scrape Instagram](https://scrapfly.io/blog/how-to-scrape-instagram/) |
| [Leboncoin.fr](./leboncoin-scraper/) | [How to Scrape Leboncoin.fr](https://scrapfly.io/blog/how-to-scrape-leboncoin-marketplace-real-estate/) |
| [LinkedIn.com](./linkedin-scraper/) | [How to Scrape LinkedIn](https://scrapfly.io/blog/how-to-scrape-linkedin-person-profile-company-job-data/) |
| [Nordstrom.com](./nordstorm-scraper/) | [How to Scrape Nordstrom](https://scrapfly.io/blog/how-to-scrape-nordstrom/) |
| [Realestate.com.au](./realestatecom-scraper/) | [How to Scrape Realestate.com.au](https://scrapfly.io/blog/how-to-scrape-realestate-com-au-property-listing-data/) |
| [Realtor.com](./realtorcom-scraper/) | [How to Scrape Realtor.com](https://scrapfly.io/blog/how-to-scrape-realtorcom/) |
| [Reddit.com](./reddit-scraper/) | [How to Scrape Reddit](https://scrapfly.io/blog/how-to-scrape-reddit-social-data/) |
| [Redfin.com](./redfin-scraper/) | [How to Scrape Redfin](https://scrapfly.io/blog/how-to-scrape-redfin/) |
| [Rightmove.com](./rightmove-scraper/) | [How to Scrape RightMove](https://scrapfly.io/blog/how-to-scrape-rightmove/) |
| [Seloger.com](./seloger-scraper/) | [How to Scrape Seloger.com](https://scrapfly.io/blog/how-to-scrape-seloger-com-listing-real-estate-ads/) |
| [Similarweb.com](./similarweb-scraper/) | [How to Scrape SimilarWeb](https://scrapfly.io/blog/how-to-scrape-similarweb/) |
| [Stockx.com](./stockx-scraper/) | [How to Scrape StockX](https://scrapfly.io/blog/how-to-scrape-stockx/) |
| [Threads.net](./threads-scraper/) | [How to Scrape Threads](https://scrapfly.io/blog/how-to-scrape-threads/) |
| [TikTok.com](./tiktok-scraper/) | [How to Scrape TikTok](https://scrapfly.io/blog/how-to-scrape-tiktok-python-json/) |
| [Tripadvisor.com](./tripadvisor-scraper/) | [How to Scrape TripAdvisor](https://scrapfly.io/blog/how-to-scrape-tripadvisor/) |
| [Trustpilot.com](./trustpilot-scraper/) | [How to Scrape Trustpilot](https://scrapfly.io/blog/how-to-scrape-trustpilot-com-reviews/) |
| [Twitter/X.com](./twitter-scraper/) | [How to Scrape X.com](https://scrapfly.io/blog/how-to-scrape-twitter/) |
| [VestiaireCollective.com](./vestiairecollective-scraper/) | [How to Scrape Vestiaire Collective](https://scrapfly.io/blog/how-to-scrape-vestiairecollective/) |
| [Walmart.com](./walmart-scraper/) | [How to Scrape Walmart.com](https://scrapfly.io/blog/how-to-scrape-walmartcom/) |
| [Wellfound.com](./wellfound-scraper/) | [How to Scrape Wellfound](https://scrapfly.io/blog/how-to-scrape-wellfound-aka-angellist/) |
| [Yellowpages.com](./yellowpages-scraper/) | [How to Scrape YellowPages.com](https://scrapfly.io/blog/how-to-scrape-yellowpages/) |
| [Yelp.com](./yelp-scraper/) | [How to Scrape Yelp.com](https://scrapfly.io/blog/how-to-scrape-yelpcom/) |
| [YouTube.com](./youtube-scraper/) | [How to Scrape YouTube](https://scrapfly.io/blog/posts/how-to-scrape-youtube-in-2025) |
| [Zillow.com](./zillow-scraper/) | [How to Scrape Zillow](https://scrapfly.io/blog/how-to-scrape-zillow/) |
| [Zoominfo.com](./zoominfo-scraper/) | [How to Scrape Zoominfo](https://scrapfly.io/blog/how-to-scrape-zoominfo/) |
| [Zoopla.co.uk](./zoopla-scraper/) | [How to Scrape Zoopla](https://scrapfly.io/blog/how-to-scrape-zoopla/) |

## Credits

- **Original Scrapers**: [ScrapFly](https://github.com/scrapfly/scrapfly-scrapers)
- **Scrappey Wrapper**: Adapted to use [Scrappey API](https://scrappey.com)

## Fair Use and Legal Disclaimer

This repository contains _educational_ reference material to illustrate how accessible web scraping can be and the provided programs are not intended to be used in web scraping production.

Consult a lawyer when creating programs that interact with other people's websites. Here's a good general intro of what NOT to do:
- Do not store PII (personally identifiable information) of EU citizens who are protected by GDPR.
- Do not scrape and repurpose entire public datasets which can be protected by database protection laws in some countries.
- Do not scrape at rates that could damage the website and scrape only publicly available data.
